{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import punkt\n"
      ],
      "metadata": {
        "id": "wb0yfhiUaZyK"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "nltk.download('stopwords')\n"
      ],
      "metadata": {
        "id": "sCgBZi4tRF_A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "HaQMxc1TMc6w"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup as bs\n",
        "import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import threading\n",
        "import os\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwBkXHZFQl8f",
        "outputId": "b5034f4e-9fb5-444b-f524-64f5f1540f80"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('Input.csv')"
      ],
      "metadata": {
        "id": "-zkjiqywNpIP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_urls = []\n",
        "error_ids = []\n",
        "def article(url,url_id,fpath):\n",
        "  try:\n",
        "    ht = requests.get(url).text\n",
        "    soup = bs(ht,'lxml')\n",
        "    div = soup.find('div',class_='td-post-content')\n",
        "    h1 = soup.find('h1')\n",
        "    title = h1.text\n",
        "    arti = div.text\n",
        "    if div==None or arti==None:\n",
        "      print(url)\n",
        "    text = title + '.' + arti\n",
        "    with open(f'{fpath+str(url_id)}.txt','w') as fp:\n",
        "      fp.write(text)\n",
        "\n",
        "\n",
        "\n",
        "  except:\n",
        "    global error_urls\n",
        "    global error_ids\n",
        "    print(url)\n",
        "    error_urls.append(url)\n",
        "    error_ids.append(url_id)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "article('https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/',1,'sample_data/Articles/')"
      ],
      "metadata": {
        "id": "4os1EywqesBp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a90705-d68c-4b1d-e2ce-b71b9ddea172"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_set(fpath):\n",
        "  s = set()\n",
        "  with open(fpath,'r') as fp:\n",
        "    content = fp.read()\n",
        "  words = content.split('\\n')\n",
        "  for i in words:\n",
        "    s.add(i.lower())\n",
        "  s.discard('')\n",
        "  return s\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qDkdPpHl9q0m"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_set2(fpath):\n",
        "    ss = set()\n",
        "    with open(fpath,'r') as f:\n",
        "     content2 = f.read()\n",
        "    word_raw = content2.split('\\n')\n",
        "    for ii in word_raw:\n",
        "      if '|' in ii:\n",
        "        ss.add(ii.split('|')[0].rstrip().lower())\n",
        "      else:\n",
        "        ss.add(ii.lower())\n",
        "    ss.discard('')\n",
        "    return ss\n"
      ],
      "metadata": {
        "id": "040alIc3ClV8"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.mkdir('articles')"
      ],
      "metadata": {
        "id": "go4ms6QQQxwy"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "with open('positive.json','r') as jp:\n",
        "  pos = json.load(jp)\n",
        "  pos = set(pos)\n",
        "with open('negative.json','r') as jp1:\n",
        "  neg = json.load(jp1)\n",
        "  neg = set(neg)\n",
        "with open('stop_words.json','r') as jp2:\n",
        "  stop = json.load(jp2)\n",
        "  stop_words = set(stop)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "bWBx564xbCUv"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-LledNDaPZM",
        "outputId": "e2609558-8378-4ca6-e491-6aba59e31189"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def rem_punkt(text):\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "\n",
        "    # Use the translation table to remove punctuation from the text\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "\n",
        "    return text_without_punctuation\n",
        "\n"
      ],
      "metadata": {
        "id": "pCS7mSb_uNl6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def is_num(s):\n",
        "    try:\n",
        "        float_num = float(s)\n",
        "        return True\n",
        "    except ValueError:\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "def sentiment_analysis(content,avoid_numbers = False):\n",
        "\n",
        "  pattern = re.compile(r'[!#$%&()*+,./:;<=>?\\\\@\\[\\]_{|}~]')\n",
        "  sentences = nltk.sent_tokenize(content)\n",
        "  pos_count = 0\n",
        "  neg_count = 0\n",
        "  word_count = 0\n",
        "  for s in sentences:\n",
        "    sentence = pattern.sub(' ',s)\n",
        "    word_list = sentence.split()\n",
        "    for word in word_list:\n",
        "      if word.lower() in stop_words:\n",
        "        continue\n",
        "      elif avoid_numbers and is_num(word):\n",
        "        continue\n",
        "      elif word.lower() in pos:\n",
        "        pos_count += 1\n",
        "        word_count +=1\n",
        "      elif word.lower() in neg:\n",
        "        neg_count += 1\n",
        "        word_count += 1\n",
        "      else:\n",
        "        word_count += 1\n",
        "  polarity_score = (pos_count-neg_count)/((pos_count+neg_count)+0.000001)\n",
        "  subjectivity_score = (pos_count+neg_count)/(word_count + 0.000001)\n",
        "  return [pos_count,neg_count,polarity_score,subjectivity_score]\n",
        "\n"
      ],
      "metadata": {
        "id": "z9yUE9a2a9fA"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def syllable(s):\n",
        "  count = 0\n",
        "  vowels = ['a','e','i','o','u']\n",
        "  vowels = set(vowels)\n",
        "  for i in s:\n",
        "    if i in vowels:\n",
        "      count += 1\n",
        "  if len(s)>1:\n",
        "    if s[-2]=='e':\n",
        "      if s[-1]=='d' or s[-1]=='s':\n",
        "        count = count - 1\n",
        "  return count\n",
        "\n",
        "\n",
        "def sentence_analysis(content):\n",
        "  pronoun_count = 0\n",
        "  word_count = 0\n",
        "  complex_count = 0\n",
        "  syllable_count = 0\n",
        "  letter_count = 0\n",
        "  personal_pronouns = ['I','me','Me','Us','us','Ours','ours','we','We']\n",
        "  pattern = re.compile(r'[!#$&()*+,./:;<=>?\\\\@\\[\\]{|}~]')\n",
        "  sentences = nltk.sent_tokenize(content)\n",
        "  total_sentences = len(sentences)\n",
        "  for sent in sentences:\n",
        "    sent = pattern.sub(' ',sent)\n",
        "    words = sent.split()\n",
        "    for word in words:\n",
        "      if word.lower() in stopwords.words('english'):\n",
        "        continue\n",
        "      else:\n",
        "        word = rem_punkt(word)\n",
        "        if word!='':\n",
        "          word_count += 1\n",
        "          letter_count += len(word)\n",
        "          syllab = syllable(word.lower())\n",
        "          if syllab>2:\n",
        "            complex_count +=1\n",
        "          syllable_count += syllab\n",
        "          if word in personal_pronouns:\n",
        "            pronoun_count += 1\n",
        "  avg_sentence_len = word_count/total_sentences\n",
        "  per_complex_words = (complex_count/word_count)*100\n",
        "  avg_word_len = word_count/letter_count\n",
        "  fog_index = (avg_sentence_len+per_complex_words)*0.4\n",
        "  word_per_sent = word_count/total_sentences\n",
        "  syllable_per_word = syllable_count/word_count\n",
        "  return [avg_sentence_len, per_complex_words, fog_index, word_per_sent, complex_count, word_count, syllable_per_word,pronoun_count,avg_word_len]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8U5QASTXqNx_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uMWK9HxV0XAu"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error_urls1 = []\n",
        "error_ids1 = []\n",
        "def final(i):\n",
        "  global df2\n",
        "  try:\n",
        "    url = df2['URL'][i]\n",
        "    url_id = df2['URL_ID'][i]\n",
        "    ht = requests.get(url).text\n",
        "    soup = bs(ht,'lxml')\n",
        "    div = soup.find('div',class_='td-post-content')\n",
        "    h1 = soup.find('h1')\n",
        "    title = h1.text\n",
        "    arti = div.text\n",
        "    if div==None or arti==None:\n",
        "      print(url)\n",
        "    text = title + '.' + arti\n",
        "    fpath = './articles/'\n",
        "    with open(f'{fpath+str(url_id)}.txt','w') as fp:\n",
        "      fp.write(text)\n",
        "    pos_count,neg_count,polarity_score,subjectivity_score = sentiment_analysis(text)\n",
        "    df2.loc[i,'POSITIVE SCORE'] = pos_count\n",
        "    df2.loc[i,'NEGATIVE SCORE'] = neg_count\n",
        "    df2.loc[i,'POLARITY SCORE'] = polarity_score\n",
        "    df2.loc[i,'SUBJECTIVITY SCORE'] = subjectivity_score\n",
        "    anslis = sentence_analysis(text)\n",
        "    df2.loc[i,'AVG SENTENCE LENGTH'] = anslis[0]\n",
        "    df2.loc[i,'PERCENTAGE OF COMPLEX WORDS'] = anslis[1]\n",
        "    df2.loc[i,'FOG INDEX'] = anslis[2]\n",
        "    df2.loc[i,'AVG NUMBER OF WORDS PER SENTENCE'] = anslis[3]\n",
        "    df2.loc[i,'COMPLEX WORD COUNT'] = anslis[4]\n",
        "    df2.loc[i,'WORD COUNT'] = anslis[5]\n",
        "    df2.loc[i,'SYLLABLE PER WORD'] = anslis[6]\n",
        "    df2.loc[i,'PERSONAL PRONOUNS'] = anslis[7]\n",
        "    df2.loc[i,'AVG WORD LENGTH'] = anslis[8]\n",
        "\n",
        "\n",
        "  except:\n",
        "    global error_urls1\n",
        "    global error_ids1\n",
        "    print(url)\n",
        "    error_urls1.append(url)\n",
        "    error_ids1.append(url_id)\n"
      ],
      "metadata": {
        "id": "yd0RR_7TFNlE"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = pd.read_csv('output_struct.csv')\n",
        "df2.columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qWTexrTlF9J1",
        "outputId": "758a32dc-8c66-4832-d194-ba7b87f3f0db"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['URL_ID', 'URL', 'POSITIVE SCORE', 'NEGATIVE SCORE', 'POLARITY SCORE',\n",
              "       'SUBJECTIVITY SCORE', 'AVG SENTENCE LENGTH',\n",
              "       'PERCENTAGE OF COMPLEX WORDS', 'FOG INDEX',\n",
              "       'AVG NUMBER OF WORDS PER SENTENCE', 'COMPLEX WORD COUNT', 'WORD COUNT',\n",
              "       'SYLLABLE PER WORD', 'PERSONAL PRONOUNS', 'AVG WORD LENGTH'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for oo in range(len(df2)):\n",
        "  final(oo)\n",
        "df2.to_csv('output.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fQxn27VIVfRi",
        "outputId": "3a9beae3-12b0-43a5-e678-2e3101de5587"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "836\n",
            "285\n",
            "521\n",
            "644\n",
            "644\n",
            "575\n",
            "558\n",
            "836\n",
            "209\n",
            "511\n",
            "177\n",
            "589\n",
            "276\n",
            "471\n",
            "466\n",
            "552\n",
            "546\n",
            "966\n",
            "574\n",
            "827\n",
            "651\n",
            "809\n",
            "575\n",
            "335\n",
            "https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
            "329\n",
            "933\n",
            "1011\n",
            "582\n",
            "603\n",
            "658\n",
            "746\n",
            "249\n",
            "711\n",
            "458\n",
            "363\n",
            "147\n",
            "https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
            "655\n",
            "319\n",
            "761\n",
            "325\n",
            "194\n",
            "548\n",
            "328\n",
            "150\n",
            "87\n",
            "344\n",
            "81\n",
            "616\n",
            "236\n",
            "455\n",
            "809\n",
            "704\n",
            "627\n",
            "318\n",
            "499\n",
            "168\n",
            "722\n",
            "405\n",
            "448\n",
            "579\n",
            "710\n",
            "431\n",
            "660\n",
            "365\n",
            "588\n",
            "782\n",
            "1619\n",
            "769\n",
            "815\n",
            "68\n",
            "452\n",
            "685\n",
            "801\n",
            "486\n",
            "911\n",
            "449\n",
            "506\n",
            "464\n",
            "673\n",
            "73\n",
            "492\n",
            "335\n",
            "544\n",
            "410\n",
            "215\n",
            "279\n",
            "527\n",
            "240\n",
            "507\n",
            "465\n",
            "516\n",
            "588\n",
            "697\n",
            "923\n",
            "357\n",
            "929\n",
            "510\n",
            "191\n",
            "1098\n",
            "549\n",
            "796\n",
            "795\n",
            "1046\n",
            "818\n",
            "391\n",
            "786\n",
            "408\n",
            "354\n",
            "831\n",
            "888\n",
            "635\n",
            "523\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "folder_to_download = 'articles'\n",
        "output_zip_file = 'articles.zip'\n",
        "\n",
        "shutil.make_archive(output_zip_file.split('.')[0], 'zip', folder_to_download)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gdo4tMlxjux5",
        "outputId": "b1398da4-5ae0-4341-ca35-43ba53a9ab7c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/articles.zip'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(error_ids1,error_urls1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7nq0x3FJV4g9",
        "outputId": "bbed406e-e878-4f7d-a178-4a2e7c7d0b52"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[] []\n"
          ]
        }
      ]
    }
  ]
}