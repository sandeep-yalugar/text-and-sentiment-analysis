# -*- coding: utf-8 -*-
"""BlackCopper.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lL88yOFJV4YaCJz0nDIyN8fFCAdpIjFU
"""

from nltk.corpus import stopwords
from nltk.tokenize import punkt

import re
nltk.download('stopwords')

from bs4 import BeautifulSoup as bs
import requests
import pandas as pd
import time
import threading
import os
import json

import nltk
nltk.download('punkt')

df = pd.read_csv('Input.csv')

error_urls = []
error_ids = []
def article(url,url_id,fpath):
  try:
    ht = requests.get(url).text
    soup = bs(ht,'lxml')
    div = soup.find('div',class_='td-post-content')
    h1 = soup.find('h1')
    title = h1.text
    arti = div.text
    if div==None or arti==None:
      print(url)
    text = title + '.' + arti
    with open(f'{fpath+str(url_id)}.txt','w') as fp:
      fp.write(text)



  except:
    global error_urls
    global error_ids
    print(url)
    error_urls.append(url)
    error_ids.append(url_id)





article('https://insights.blackcoffer.com/rise-of-e-health-and-its-imapct-on-humans-by-the-year-2030-2/',1,'sample_data/Articles/')

def make_set(fpath):
  s = set()
  with open(fpath,'r') as fp:
    content = fp.read()
  words = content.split('\n')
  for i in words:
    s.add(i.lower())
  s.discard('')
  return s

def make_set2(fpath):
    ss = set()
    with open(fpath,'r') as f:
     content2 = f.read()
    word_raw = content2.split('\n')
    for ii in word_raw:
      if '|' in ii:
        ss.add(ii.split('|')[0].rstrip().lower())
      else:
        ss.add(ii.lower())
    ss.discard('')
    return ss

os.mkdir('articles')

with open('positive.json','r') as jp:
  pos = json.load(jp)
  pos = set(pos)
with open('negative.json','r') as jp1:
  neg = json.load(jp1)
  neg = set(neg)
with open('stop_words.json','r') as jp2:
  stop = json.load(jp2)
  stop_words = set(stop)



import string

def rem_punkt(text):
    # Create a translation table to remove punctuation
    translator = str.maketrans('', '', string.punctuation)

    # Use the translation table to remove punctuation from the text
    text_without_punctuation = text.translate(translator)

    return text_without_punctuation

def is_num(s):
    try:
        float_num = float(s)
        return True
    except ValueError:
        return False



def sentiment_analysis(content,avoid_numbers = False):

  pattern = re.compile(r'[!#$%&()*+,./:;<=>?\\@\[\]_{|}~]')
  sentences = nltk.sent_tokenize(content)
  pos_count = 0
  neg_count = 0
  word_count = 0
  for s in sentences:
    sentence = pattern.sub(' ',s)
    word_list = sentence.split()
    for word in word_list:
      if word.lower() in stop_words:
        continue
      elif avoid_numbers and is_num(word):
        continue
      elif word.lower() in pos:
        pos_count += 1
        word_count +=1
      elif word.lower() in neg:
        neg_count += 1
        word_count += 1
      else:
        word_count += 1
  polarity_score = (pos_count-neg_count)/((pos_count+neg_count)+0.000001)
  subjectivity_score = (pos_count+neg_count)/(word_count + 0.000001)
  return [pos_count,neg_count,polarity_score,subjectivity_score]

def syllable(s):
  count = 0
  vowels = ['a','e','i','o','u']
  vowels = set(vowels)
  for i in s:
    if i in vowels:
      count += 1
  if len(s)>1:
    if s[-2]=='e':
      if s[-1]=='d' or s[-1]=='s':
        count = count - 1
  return count


def sentence_analysis(content):
  pronoun_count = 0
  word_count = 0
  complex_count = 0
  syllable_count = 0
  letter_count = 0
  personal_pronouns = ['I','me','Me','Us','us','Ours','ours','we','We']
  pattern = re.compile(r'[!#$&()*+,./:;<=>?\\@\[\]{|}~]')
  sentences = nltk.sent_tokenize(content)
  total_sentences = len(sentences)
  for sent in sentences:
    sent = pattern.sub(' ',sent)
    words = sent.split()
    for word in words:
      if word.lower() in stopwords.words('english'):
        continue
      else:
        word = rem_punkt(word)
        if word!='':
          word_count += 1
          letter_count += len(word)
          syllab = syllable(word.lower())
          if syllab>2:
            complex_count +=1
          syllable_count += syllab
          if word in personal_pronouns:
            pronoun_count += 1
  avg_sentence_len = word_count/total_sentences
  per_complex_words = (complex_count/word_count)*100
  avg_word_len = word_count/letter_count
  fog_index = (avg_sentence_len+per_complex_words)*0.4
  word_per_sent = word_count/total_sentences
  syllable_per_word = syllable_count/word_count
  return [avg_sentence_len, per_complex_words, fog_index, word_per_sent, complex_count, word_count, syllable_per_word,pronoun_count,avg_word_len]



error_urls1 = []
error_ids1 = []
def final(i):
  global df2
  try:
    url = df2['URL'][i]
    url_id = df2['URL_ID'][i]
    ht = requests.get(url).text
    soup = bs(ht,'lxml')
    div = soup.find('div',class_='td-post-content')
    h1 = soup.find('h1')
    title = h1.text
    arti = div.text
    if div==None or arti==None:
      print(url)
    text = title + '.' + arti
    fpath = './articles/'
    with open(f'{fpath+str(url_id)}.txt','w') as fp:
      fp.write(text)
    pos_count,neg_count,polarity_score,subjectivity_score = sentiment_analysis(text)
    df2.loc[i,'POSITIVE SCORE'] = pos_count
    df2.loc[i,'NEGATIVE SCORE'] = neg_count
    df2.loc[i,'POLARITY SCORE'] = polarity_score
    df2.loc[i,'SUBJECTIVITY SCORE'] = subjectivity_score
    anslis = sentence_analysis(text)
    df2.loc[i,'AVG SENTENCE LENGTH'] = anslis[0]
    df2.loc[i,'PERCENTAGE OF COMPLEX WORDS'] = anslis[1]
    df2.loc[i,'FOG INDEX'] = anslis[2]
    df2.loc[i,'AVG NUMBER OF WORDS PER SENTENCE'] = anslis[3]
    df2.loc[i,'COMPLEX WORD COUNT'] = anslis[4]
    df2.loc[i,'WORD COUNT'] = anslis[5]
    df2.loc[i,'SYLLABLE PER WORD'] = anslis[6]
    df2.loc[i,'PERSONAL PRONOUNS'] = anslis[7]
    df2.loc[i,'AVG WORD LENGTH'] = anslis[8]


  except:
    global error_urls1
    global error_ids1
    print(url)
    error_urls1.append(url)
    error_ids1.append(url_id)

df2 = pd.read_csv('output_struct.csv')
df2.columns

for oo in range(len(df2)):
  final(oo)
df2.to_csv('output.csv')

import shutil

folder_to_download = 'articles'
output_zip_file = 'articles.zip'

shutil.make_archive(output_zip_file.split('.')[0], 'zip', folder_to_download)

print(error_ids1,error_urls1)